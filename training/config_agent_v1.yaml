project_name: "IndraAgent-V1"
run_name: "IndraAgent-V1-Distill"

model:
  vocab_size: 151936 # Qwen 2.5
  d_model: 512
  num_layers: 8
  num_heads: 8
  d_ff: 2048
  dropout: 0.1
  max_seq_len: 256 # Back to 256 to fit EMA + Teacher
  tie_word_embeddings: true # CRITICAL for VRAM
  gradient_checkpointing: false # Disable for speed (we have enough VRAM with 8-bit optimizer)
  window_size: null # Set to e.g. 256 to enable Sliding Window Attention. null = Full Attention.

training:
  batch_size: 1
  accumulate_grad_batches: 32 # Increased for stability (Effective Batch = 32)
  learning_rate: 1.0e-4 # Increased for faster convergence with larger batch
  max_steps: 50000
  warmup_steps: 500 # Reduced from 1000 for faster convergence
  save_every: 100 # Save checkpoint every 100 optimizer steps (was 2000 batch steps)
  log_every: 10 # Log every 10 optimizer steps
  use_8bit_optimizer: true
  use_ema: false # Disabled to save 0.8GB Shared Memory (Max Speed)
  ema_decay: 0.9999

distillation:
  teacher_model: "Qwen/Qwen2.5-0.5B-Instruct"
  temperature: 2.0
  alpha_ce: 0.5
  alpha_kd: 0.5

data:
  dataset_name: "teknium/OpenHermes-2.5" # Real Agent Data
  subset_name: null 
  num_proc: 4
