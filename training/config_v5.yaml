project_name: "IndraQuantum-V5"
run_name: "IndraV5-Tiny-Initialization"

model:
  vocab_size: 32000 # Typical Llama/Mistral size or custom
  d_model: 256
  num_layers: 4
  num_heads: 4
  d_ff: 1024
  dropout: 0.1
  max_seq_len: 256

training:
  batch_size: 2 # Small batch for test
  accumulate_grad_batches: 1
  learning_rate: 3.0e-4
  max_steps: 5
  warmup_steps: 1
  log_every: 1
  save_every: 5

loss:
  rho_mag: 0.1
  rho_phase: 0.5
  use_teacher_mag: false

data:
  dataset_name: "wikitext" # or path to data
  dataset_config: "wikitext-2-v1" # subset
