project_name: "IndraQuantum-V7"
run_name: "IndraV7-Nano-Balanced"

model:
  vocab_size: 50257 # Standard GPT2 Tokenizer
  d_model: 256
  num_layers: 6      # Deeper for Reasoning (Nano size)
  num_heads: 8       # 32 dim per head
  d_ff: 1024         # 4x expansion
  dropout: 0.2       # Increase dropout to prevent memorization
  max_seq_len: 512

training:
  batch_size: 2       # Reduce to fit in 6GB VRAM (Avoid thrashing)
  accumulate_grad_batches: 16 # 2 * 16 = 32 effective batch size
  learning_rate: 3.0e-4 # Slower, more stable learning
  max_steps: 10000    # Give it more time to find good minima
  warmup_steps: 500   # Longer warmup
  log_every: 50
  save_every: 1000
  
loss:
  rho_mag: 0.1
  rho_phase: 0.0      # Disabled to verify backbone capability
  use_teacher_mag: false

data:
  dataset_name: "roneneldan/TinyStories"
  dataset_config: null # Default configuration
  cache_dir: "data/datasets" # Download to local project folder

generation:
  prompts:
    - "Once upon a time"
    - "One day, a little girl named Alice"
    - "The cat sat on the mat"
  max_new_tokens: 100
  temperature: 0.7
  top_k: 50
  repetition_penalty: 1.2 # Prevent loops
  device: "auto" # auto, cpu, cuda
