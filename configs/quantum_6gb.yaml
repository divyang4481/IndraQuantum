# IndraQuantum Configuration
# Optimized for 6GB VRAM consumer GPUs

# Model Architecture
d_model: 128 # Dimension of complex embeddings (real + imaginary)
vocab_size: 50257 # Vocabulary size (Matched to GPT-2 for KD)
n_layers: 4 # Number of quantum layers
n_heads: 4 # Number of attention heads (for future multi-head implementation)
dropout: 0.1 # Dropout probability
max_seq_length: 2048 # Maximum sequence length (Increased from 512)

# Training Hyperparameters
batch_size: 4 # Batch size (optimized for 6GB VRAM)
learning_rate: 3.0e-4 # Initial learning rate
weight_decay: 0.01 # Weight decay for regularization
max_grad_norm: 1.0 # Gradient clipping threshold
num_epochs: 10 # Number of training epochs
min_lr: 1.0e-6 # Minimum learning rate for scheduler

# Knowledge Distillation
temperature: 2.0 # Temperature for softening probability distributions
alpha: 0.5 # Weight for distillation loss (0.5 = equal weight with task loss)

# Data
num_workers: 0 # Number of data loading workers (0 = main process only)

# Checkpoint
save_every: 1 # Save checkpoint every N epochs

# Hardware Optimization
mixed_precision: true # Use mixed precision training (FP16)
gradient_accumulation_steps: 1 # Accumulate gradients over N steps
pin_memory: true # Pin memory for faster data transfer

# Phase Shift Configuration (for graph module)
num_phases: 8 # Number of learnable phase parameters
use_graph: false # Enable graph-based phase computation

# Memory Management
empty_cache_every_n_steps: 100 # Clear CUDA cache every N steps
