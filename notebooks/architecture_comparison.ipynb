{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b551cd86",
   "metadata": {},
   "source": [
    "# IndraQuantum: Architecture Analysis & Comparison\n",
    "\n",
    "This notebook analyzes the performance and characteristics of the **IndraQuantum** model compared to a standard baseline.\n",
    "It also explores the **Quantum Graph** structure where text is modeled as a hierarchy of quantum systems (Tokens -> Sentences -> Paragraphs).\n",
    "\n",
    "## 1. Initialization Analysis\n",
    "We compare the \"Quantum Polar Initialization\" (Magnitude ~ 1, Random Phase) vs Standard Gaussian Initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb791c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from indra.models.embedding import QuantumEmbedding\n",
    "\n",
    "def analyze_initialization(vocab_size=1000, d_model=64):\n",
    "    print(\"Analyzing Initialization...\")\n",
    "    \n",
    "    # Quantum Embedding\n",
    "    q_emb = QuantumEmbedding(vocab_size, d_model)\n",
    "    q_weights = q_emb.embedding.weight.detach()\n",
    "    real, imag = torch.chunk(q_weights, 2, dim=-1)\n",
    "    q_mags = torch.sqrt(real**2 + imag**2).flatten().numpy()\n",
    "    q_phases = torch.atan2(imag, real).flatten().numpy()\n",
    "    \n",
    "    # Standard Embedding\n",
    "    s_emb = nn.Embedding(vocab_size, d_model * 2)\n",
    "    s_weights = s_emb.weight.detach()\n",
    "    s_real, s_imag = torch.chunk(s_weights, 2, dim=-1)\n",
    "    s_mags = torch.sqrt(s_real**2 + s_imag**2).flatten().numpy()\n",
    "    s_phases = torch.atan2(s_imag, s_real).flatten().numpy()\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    axs[0].hist(q_mags, bins=50, alpha=0.7, label='Quantum (Polar Init)', color='blue')\n",
    "    axs[0].hist(s_mags, bins=50, alpha=0.7, label='Standard (Normal Init)', color='orange')\n",
    "    axs[0].set_title(\"Initialization Magnitude Distribution\")\n",
    "    axs[0].set_xlabel(\"Magnitude\")\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].hist(q_phases, bins=50, alpha=0.7, label='Quantum', color='blue')\n",
    "    axs[1].hist(s_phases, bins=50, alpha=0.7, label='Standard', color='orange')\n",
    "    axs[1].set_title(\"Initialization Phase Distribution\")\n",
    "    axs[1].set_xlabel(\"Phase (Radians)\")\n",
    "    axs[1].legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "analyze_initialization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0484bbb7",
   "metadata": {},
   "source": [
    "## 2. Training Performance Comparison\n",
    "We compare the training loss curves of:\n",
    "1. **IndraQuantum**: Complex-valued MLP with Quantum Embeddings.\n",
    "2. **Standard Baseline**: Real-valued MLP with same parameter count.\n",
    "3. **IndraQuantum Graph**: Graph-based model with Token-Sentence-Paragraph hierarchy.\n",
    "\n",
    "*Note: Models are trained using Knowledge Distillation from a TinyLlama teacher.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccacad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "def plot_training_comparison():\n",
    "    log_dir = \"../logs\"\n",
    "    data = {}\n",
    "    \n",
    "    # Load Data\n",
    "    files = {\n",
    "        'IndraQuantum': 'history_quantum.pkl',\n",
    "        'Baseline': 'history_baseline.pkl',\n",
    "        'IndraQuantum-Graph': 'history_graph.pkl'\n",
    "    }\n",
    "    \n",
    "    for name, filename in files.items():\n",
    "        path = os.path.join(log_dir, filename)\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"rb\") as f:\n",
    "                history = pickle.load(f)\n",
    "                # Handle different formats if any (some might be dict, some list)\n",
    "                if isinstance(history, dict):\n",
    "                    data[name] = history['loss']\n",
    "                else:\n",
    "                    data[name] = history\n",
    "        else:\n",
    "            print(f\"Warning: {filename} not found in {log_dir}\")\n",
    "\n",
    "    if not data:\n",
    "        print(\"No training data found. Please run the training scripts first.\")\n",
    "        return\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, losses in data.items():\n",
    "        plt.plot(losses, label=name, marker='o', linewidth=2)\n",
    "        \n",
    "    plt.title(\"Training Loss Comparison (Knowledge Distillation)\", fontsize=14)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Loss (KL Divergence)\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save plot to plots folder as well\n",
    "    if not os.path.exists(\"../plots\"):\n",
    "        os.makedirs(\"../plots\")\n",
    "    plt.savefig(\"../plots/notebook_comparison_plot.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_training_comparison()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904254f",
   "metadata": {},
   "source": [
    "## 3. Performance Benchmark\n",
    "We dynamically calculate the **Parameters**, **Inference Latency**, and **Peak Memory Usage** for the models.\n",
    "This ensures we are looking at real metrics on the current hardware, rather than hardcoded estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55dc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from indra.models.quantum_core import IndraQuantum\n",
    "from indra.models.baseline import StandardBaseline\n",
    "from indra.models.quantum_graph import IndraQuantumGraph\n",
    "\n",
    "def measure_metrics(model_name, model, input_shape, device='cuda'):\n",
    "    print(f\"Benchmarking {model_name}...\")\n",
    "    # Parameters\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Move to device\n",
    "    try:\n",
    "        model = model.to(device)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to move {model_name} to {device}: {e}\")\n",
    "        return None\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = torch.randint(0, 1000, input_shape).to(device)\n",
    "    \n",
    "    # Handle Graph Model Inputs\n",
    "    if isinstance(model, IndraQuantumGraph):\n",
    "        node_types = torch.zeros(input_shape, dtype=torch.long).to(device)\n",
    "        # Dummy graph mask: [B, S, S] - Identity for minimal overhead test\n",
    "        graph_mask = torch.eye(input_shape[1]).unsqueeze(0).expand(input_shape[0], -1, -1).to(device)\n",
    "        inputs = (input_ids, node_types, graph_mask)\n",
    "    else:\n",
    "        inputs = (input_ids,)\n",
    "        \n",
    "    # Warmup\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for _ in range(5):\n",
    "                model(*inputs)\n",
    "    except Exception as e:\n",
    "        print(f\"Warmup failed for {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Latency\n",
    "    torch.cuda.synchronize() if device == 'cuda' else None\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(20): # 20 runs for speed\n",
    "            model(*inputs)\n",
    "    torch.cuda.synchronize() if device == 'cuda' else None\n",
    "    end = time.time()\n",
    "    latency = (end - start) / 20 * 1000 # ms\n",
    "    \n",
    "    # Memory (Peak Allocated)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        with torch.no_grad():\n",
    "            model(*inputs)\n",
    "        memory = torch.cuda.max_memory_allocated() / 1024 / 1024 # MB\n",
    "    else:\n",
    "        memory = 0 # Cannot easily measure CPU memory this way\n",
    "    \n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Parameters\": params,\n",
    "        \"Latency (ms)\": latency,\n",
    "        \"Memory (MB)\": memory\n",
    "    }\n",
    "\n",
    "# Config\n",
    "vocab_size = 32000\n",
    "d_model = 128\n",
    "seq_len = 128 \n",
    "batch_size = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate Models\n",
    "models = [\n",
    "    (\"Standard Baseline\", StandardBaseline(vocab_size, d_model)),\n",
    "    (\"Indra Quantum\", IndraQuantum(vocab_size, d_model)),\n",
    "    (\"Indra Graph\", IndraQuantumGraph(vocab_size, d_model))\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, model in models:\n",
    "    res = measure_metrics(name, model, (batch_size, seq_len), device)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# Create DataFrame\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(df)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Params\n",
    "    axes[0].bar(df[\"Model\"], df[\"Parameters\"], color=['orange', 'blue', 'purple'])\n",
    "    axes[0].set_title(\"Parameter Count\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "    # Latency\n",
    "    axes[1].bar(df[\"Model\"], df[\"Latency (ms)\"], color=['orange', 'blue', 'purple'])\n",
    "    axes[1].set_title(\"Inference Latency (ms)\")\n",
    "    axes[1].set_ylabel(\"Time (ms)\")\n",
    "\n",
    "    # Memory\n",
    "    axes[2].bar(df[\"Model\"], df[\"Memory (MB)\"], color=['orange', 'blue', 'purple'])\n",
    "    axes[2].set_title(\"Peak Memory Usage (MB)\")\n",
    "    axes[2].set_ylabel(\"Memory (MB)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if not os.path.exists(\"../plots\"):\n",
    "        os.makedirs(\"../plots\")\n",
    "    plt.savefig(\"../plots/benchmark_metrics.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29eef8",
   "metadata": {},
   "source": [
    "## 4. Graph Model Analysis\n",
    "We analyze the **Learned Edge Biases** of the trained `IndraQuantumGraph` model.\n",
    "The model has a learnable bias for different edge types:\n",
    "1. **Local Bias**: Attention to nearby tokens (sliding window).\n",
    "2. **Hierarchy Bias**: Attention along structural edges (Token <-> Sentence <-> Paragraph).\n",
    "\n",
    "If the **Hierarchy Bias** is non-zero (and distinct from zero), it confirms the model is **actively using the graph structure** during processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c699d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from indra.models.quantum_graph import IndraQuantumGraph\n",
    "\n",
    "def analyze_graph_weights():\n",
    "    print(\"Analyzing Graph Model Weights...\")\n",
    "    \n",
    "    checkpoint_path = \"../checkpoints/quantum_graph.pt\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint not found at {checkpoint_path}. Run scripts/train_graph.py first.\")\n",
    "        return\n",
    "\n",
    "    # Initialize Model (Same config as training)\n",
    "    vocab_size = 32002 # 32000 + 2 special tokens\n",
    "    d_model = 128\n",
    "    model = IndraQuantumGraph(vocab_size, d_model)\n",
    "    \n",
    "    # Load Weights\n",
    "    try:\n",
    "        state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Extract Bias Weights from the first layer\n",
    "    # Shape: [n_heads, 3] -> (Local, Hierarchy, Global)\n",
    "    layer_idx = 0\n",
    "    bias_weights = model.layers[layer_idx].bias_weights.detach().numpy()\n",
    "    n_heads = bias_weights.shape[0]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = []\n",
    "    for h in range(n_heads):\n",
    "        data.append({\n",
    "            \"Head\": f\"Head {h+1}\",\n",
    "            \"Local Bias\": bias_weights[h, 0],\n",
    "            \"Hierarchy Bias\": bias_weights[h, 1]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\nLearned Bias Weights (Layer 0):\")\n",
    "    print(df)\n",
    "    \n",
    "    # Plot\n",
    "    df.plot(x=\"Head\", y=[\"Local Bias\", \"Hierarchy Bias\"], kind=\"bar\", figsize=(10, 6), color=['skyblue', 'salmon'])\n",
    "    plt.title(f\"Learned Attention Biases (Layer {layer_idx})\")\n",
    "    plt.ylabel(\"Bias Value\")\n",
    "    plt.xlabel(\"Attention Head\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    if not os.path.exists(\"../plots\"):\n",
    "        os.makedirs(\"../plots\")\n",
    "    plt.savefig(\"../plots/graph_bias_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "analyze_graph_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e2abc",
   "metadata": {},
   "source": [
    "## 5. Quantum Graph Structure\n",
    "Visualizing the adjacency matrix of the text graph.\n",
    "- **Nodes**: Tokens (0), Sentences (1), Paragraphs (2)\n",
    "- **Edges**: Hierarchical connections (Token <-> Sentence <-> Paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indra.graph.builder import TextGraphBuilder\n",
    "from transformers import AutoTokenizer\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_graph_structure():\n",
    "    # Load Tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "    except:\n",
    "        print(\"Could not load TinyLlama tokenizer, using dummy.\")\n",
    "        return\n",
    "\n",
    "    builder = TextGraphBuilder(tokenizer)\n",
    "    text = \"Quantum mechanics is fascinating. It explains the universe.\\nThis is a new paragraph. It adds more context.\"\n",
    "    \n",
    "    graph = builder.build_graph(text)\n",
    "    adj = graph['graph_mask'].numpy()\n",
    "    node_types = graph['node_types'].numpy()\n",
    "    \n",
    "    print(f\"Sequence Length: {len(node_types)}\")\n",
    "    print(f\"Node Types: {node_types}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(adj, cmap=\"viridis\", cbar=False)\n",
    "    plt.title(\"Adjacency Matrix (Graph Mask)\")\n",
    "    plt.xlabel(\"Target Node Index\")\n",
    "    plt.ylabel(\"Source Node Index\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_graph_structure()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
